{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate,PromptTemplate\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory,ConversationSummaryBufferMemory,ConversationSummaryMemory,ReadOnlySharedMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage,message_to_dict,messages_from_dict\n",
    "\n",
    "from typing import List\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "db_config = {\n",
    "    'dbname': os.environ.get('db_name'),\n",
    "    'dbuser': os.environ.get('db_username'),\n",
    "    'dbpassword': os.environ.get('db_password'),\n",
    "    'dbhost': os.environ.get('db_host'),\n",
    "    'dbport': os.environ.get('db_port'),\n",
    "}\n",
    "\n",
    "llms = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.9, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostGresMemoryHistory(BaseChatMessageHistory):\n",
    "    def __init__(self, db_name:str, db_user:str, db_password:str, db_host:str, db_port:str, table_name:str, chat_id:str):\n",
    "        self.db_config = {\n",
    "            'dbname': db_name,     \n",
    "            'user': db_user,        \n",
    "            'password': db_password,    \n",
    "            'host': db_host,           \n",
    "            'port': db_port,\n",
    "            'table_name': table_name,                 \n",
    "        }\n",
    "        \n",
    "        self.chat_id = chat_id\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            self.conn = psycopg.connect(dbname=self.db_config['dbname'], user=self.db_config['user'], password=self.db_config['password'], host=self.db_config['host'], port=self.db_config['port'])\n",
    "            self.cur = self.conn.cursor(row_factory=dict_row)\n",
    "            self.create_table_if_not_exists()\n",
    "        except psycopg.Error as e:\n",
    "            print(\"Error connecting to PostgreSQL database:\", e)\n",
    "            self.conn = None\n",
    "            self.cur = None\n",
    "                  \n",
    "    \n",
    "    def create_table_if_not_exists(self):\n",
    "        create_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS {self.db_config['table_name']} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            chat_id TEXT NOT NULL,\n",
    "            message JSONB NOT NULL, \n",
    "            time_stamp TIMESTAMP NOT NULL\n",
    "        );\"\"\"\n",
    "        self.cur.execute(create_table_query)\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def add_ai_message(self, message: str) -> None:\n",
    "        return self.add_message(AIMessage(content=message))\n",
    "    \n",
    "    def add_user_message(self, message: str) -> None:\n",
    "        return self.add_message(HumanMessage(content=message))\n",
    "    \n",
    "    @property\n",
    "    def messages(self) -> List[BaseMessage]:  # type: ignore\n",
    "        \"\"\"Retrieve the messages from PostgreSQL\"\"\"\n",
    "        query = (\n",
    "            f\"SELECT * FROM {self.db_config['table_name']} WHERE chat_id = %s ORDER BY time_stamp;\"\n",
    "        )\n",
    "        self.cur.execute(query, (self.chat_id,))\n",
    "        items = [record[\"message\"] for record in self.cur.fetchall()]\n",
    "        messages = messages_from_dict(items)\n",
    "        return messages\n",
    "    \n",
    "    \n",
    "    def add_message(self, message:BaseMessage):\n",
    "        time_stamp = datetime.now()\n",
    "        query = f\"INSERT INTO {self.db_config['table_name']} (chat_id, message, time_stamp) VALUES (%s, %s, %s);\"\n",
    "        self.cur.execute(query, (self.chat_id, json.dumps(message_to_dict(message)), time_stamp))\n",
    "        self.conn.commit()\n",
    "        \n",
    "            \n",
    "    def clear(self) -> None:\n",
    "        conn = psycopg.connect(self.db_config)\n",
    "        cur = conn.cursor()\n",
    "        if conn:\n",
    "            try:\n",
    "                query = f\"DELETE FROM {self.db_config['dbname']} WHERE session_id = %s;\"\n",
    "                cur.execute(query, (self.session_id,))\n",
    "                conn.commit()\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "            except psycopg.Error as e:\n",
    "                print(\"Error clearing DB:\", e)\n",
    "        else:\n",
    "            print(\"Failed to establish connection...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "The following is a conversation with an AI assistant and Human. The assistant is helpful, creative and very friendly. The assistant is also a great prompt engineer and AI developer.\n",
    "AI is helping human to solve AI problems regarding Contextual Learning in Large Language Models (LLM) by optimizing memory.\n",
    "\n",
    "Current Conversation History : {history}\n",
    "Human: {input}\n",
    "AI : \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_template = PromptTemplate(template=template, input_variables=[\"history\", \"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_id = '485276d7-8dab-48b1-a8ba-ad65a7108090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_memory = PostGresMemoryHistory(db_host=os.environ.get('db_host'), db_name=os.environ.get('db_name'), db_user=os.environ.get('db_username'), db_password=os.environ.get('db_password'), db_port=os.environ.get('db_port'), table_name=os.environ.get('conversation_table_name'), chat_id=chat_id)\n",
    "memory  = ConversationSummaryMemory.from_messages(chat_memory=postgres_memory, llm=llms, return_messages=True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "    llm=llms, \n",
    "    memory=memory,\n",
    "    prompt=conversation_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"Engineer for Continual Learning in LLM agents\n",
    "Testing task\n",
    "\n",
    "Objective: The goal of this task is to test your understanding of using LLMs and inventiveness through adding a prototype continual learning system to a conversational LLM agent.  \n",
    "\n",
    "Task Description:  Context length is an important factor in the learning of Large Language Models. While contexts are getting longer and longer, there has been work (https://arxiv.org/abs//2307.03172) which suggests that LLMs can fail at performing an information dependent task, even if the LLM has all of the information that it needs in its context.  \n",
    "\n",
    "This suggests that there is a need for continual learning systems. These systems augment LLMs by giving the LLM the ability to remember instructions, interactions, and relevant results from its conversations with others. For this task, you will produce a system that can resolve four issues that occur in continual learning:\n",
    "Duplicated memories: If there are repeated events, you don’t want to recall all of them. Otherwise you may fill up the context with identical events and retrievals will be uninformative.\n",
    "Contradictory memories: If there are two instructions that contradict each other, how should the LLM respond when it should execute one of the instructions? \n",
    "Temporary memories: Information with some finite lifetime (E.g “Until further notice…”)\n",
    "Episodic memories: refer to the ability of the LLM agent to remember specific events or interactions in a time-stamped manner. Unlike general rules or guidelines, these are memories tied to particular instances or conversations, allowing the LLM agent to reference past interactions for context or to provide more personalized and informed responses.\n",
    "\n",
    "This task involves using examples of four different types of information to create a solution. The key points to consider are:\n",
    "\n",
    "Utilize Provided Examples: The task comes with examples for each information type. These should be used as a guide in developing your solution.\n",
    "\n",
    "Avoid Over-Reliance on LLM Context: While the LLM has the capability to store and process information within its context, it's important not to depend too much on this feature. The assumption is that the agent (LLM) will engage in lengthy and iterative conversations over hours or days, which means it cannot retain all relevant details in its immediate context due to space limitations.\n",
    "\n",
    "Recommended LLM: It is advised to use GPT-4 for this task, considering its advanced ability to follow instructions accurately. The context size should be capped at 8,000 tokens. This limitation is intentional to ensure the necessity of an external long-term memory system, as the 8K context size is insufficient for storing all details from prolonged conversations.\n",
    "\n",
    "\n",
    "Technical Requirements:\n",
    "Conversational LLM agents augmented with a continual learning system\n",
    "Given the rich ecosystem of AI technologies that are supported, for this task you should use Python.\n",
    "Feel free to utilize any libraries or frameworks, including third-party or paid ones. The LLM that you use is anticipated to impact the quality of the continual learning, but we will also test your solution using a range of LLMs. We will weigh the approach and inventiveness of the solution higher than the LLM-dependent performance of the solution.\n",
    "Focus on creating a simple but functional prototype rather than a polished final product.\n",
    "\n",
    "We will evaluate your task based on:\n",
    "Functionality of the system.\n",
    "How well the system deals with the four types of information\n",
    "Efficiency of the implementation.\n",
    "Code quality and organization.\n",
    "How effectively you utilized third-party libraries or services to achieve the task.\n",
    "\n",
    "Please submit your code along with a brief report describing your design choices, the LLM that you used for development, any challenges you faced, how you overcame them, and a justification for the third-party tools used. Include instructions on how to run the code and any dependencies needed.\n",
    "\n",
    "This task is intended to give us an idea of your skill set, creativity, and your ability to effectively leverage existing technologies. We look forward to seeing your innovative solutions!\n",
    "\n",
    "Examples of information types:\n",
    "Duplicated information:\n",
    "\n",
    "User: I am going outside.\n",
    "Agent: Have a good time outside!\n",
    "…\n",
    "User: I have returned home and am going to take a bath\n",
    "Agent: Enjoy your bath!\n",
    "…\n",
    "User: I am going outside again.\n",
    "Agent: Thanks for letting me know, enjoy your time outside.\n",
    "\n",
    "Note: We expect to see that the memories for the user going outside are consolidated in some fashion (e.g combining both, or deleting one of the memories) so that after a large number of repetitions of an event, the memory retrieval only contains one instance of “going outside”\n",
    "\n",
    "Contradictory Information:\n",
    "\n",
    "User: Whenever I go out to my car, remind me to fill up the screenwash.\n",
    "Agent: Okay, I will remind you to fill the screenwash when you go out to your car.\n",
    "\n",
    "User: Oh, also remind me to check the oil when I go out to the car.\n",
    "Agent: Okay, I will remind you to fill your screenwash and check your oil when you go out to your car.\n",
    "…\n",
    "User: I have filled my screenwash and checked my oil. Next time I go out to my car, remind me to check my tires.\n",
    "Agent: I will remind you to check your tires the next time you go out to your car.\n",
    "\n",
    "Note: There have been three directives for reminders. The second is additive to the first, but the third one contradicts the first two because the user has performed the original tasks. If all three of these events are in the LTM, which one would be chosen? We do not want the agent to remind the user about the screenwash and oil, but only the tires. \n",
    "\n",
    "\n",
    "Not Contradictory information:\n",
    "\n",
    "User: Hey, can you set up a reminder for me?\n",
    "Agent: Sure, what would you like to be reminded of?\n",
    "\n",
    "User: I want to be reminded to leave the house at 2pm today.\n",
    "Agent: Alright, I've set a reminder for you to leave the house at 2pm today. Anything else you need?\n",
    "\n",
    "User: Will you remind me again, when I have to leave the house at 6pm this evening?\n",
    "Agent: Absolutely, I've also set a reminder for you to leave the house at 6pm today. \n",
    "\n",
    "Temporary Information:\n",
    "User: Remember this shopping list for me: Bread, Eggs, and Milk\n",
    "Agent: I have remembered your shopping list of Bread, Eggs, and Milk\n",
    "\n",
    "User: I have bought some eggs.\n",
    "Agent: Okay, I have removed the Eggs from your shopping list.\n",
    "…\n",
    "User: What is my shopping list again?\n",
    "Agent: On your shopping list you have: Bread and Milk\n",
    "\n",
    "Note: A shopping list is piece of information that is very volatile, so the agent and its memory should be reactive to additions and deletions, returning only the latest version of the shopping list when asked.\n",
    "\n",
    "—\n",
    "\n",
    "User: When I return home from work, suggest a meal I can make for dinner.\n",
    "Agent: I will remember to suggest a meal that you can make for dinner when you get in from work.\n",
    "…\n",
    "User: I have preplanned meals this week, so for this week, don’t make any suggestions about what to make for dinner when I get home from work.\n",
    "Agent: All right, for this week, I will not suggest meals for you when you return from work. \n",
    "…\n",
    "User: I have finished my preplanned meals, go back to suggesting meals for me.\n",
    "Agent: I will resume suggesting meals for you when you return from work\n",
    "\n",
    "Note: We have given the agent some directive in the past, then given it a contradictory instruction with an explicit understanding that this instruction will be temporary. Have the LTM deal with this temporary change of the instructions so that the agent can achieve the temporary task, and then go back to original instructions.\n",
    "\n",
    "\n",
    "\n",
    "Episodic Memory\n",
    "Two days ago:\n",
    "User: I have bought a side dresser for the bedroom, and some ingredients for dinner tonight. I am going to make omelets!\n",
    "Agent: That’s great, you needed a side dresser for your bed, and omelets are a healthy dinner!\n",
    "Today:\n",
    "User: I think I will prepare something for dinner tonight… Do you think I could use the ingredients I bought two days ago?\n",
    "Agent: The eggs should be fine, unless you actually made and ate the omelets then!\n",
    "\n",
    "—\n",
    "\n",
    "Yesterday:\n",
    "User: I have purchased some items for the house: A mirror, two potted plants and a standing lamp.\n",
    "Agent: Okay, hope you enjoy decorating your home!\n",
    "…\n",
    "Today:\n",
    "User: Yesterday I got home with some new items for the house. Do you remember what they were?\n",
    "Agent: Sure! You bought A mirror, two potted plants and a standing lamp.\n",
    "\n",
    "\n",
    "Note: Episodic retrieval allows the agent to order its memories through time, and recall or filter them using timestamps to answer questions with a time based component. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI language model, I can guide you through the conceptual design of a prototype system for continual learning in LLMs, such as GPT-4, and address the points mentioned in your task description. Below, I outline a potential approach and considerations for each type of information.\\n\\n### Design Considerations:\\n\\n#### External Memory Store:\\n- To overcome the limited context size of LLMs, we'll need an external memory store. This could be a NoSQL database, a key-value store, or any other efficient data structure that can handle the different types of memories we need to store, ensuring quick retrieval and updates.\\n\\n#### Memory Encoder:\\n- Since we have a limit on the context size, we'll need to encode memories in a way that's information-dense but also easily decodable by the LLM. This could involve summarizing events or extracting key-value pairs.\\n\\n### Dealing with Different Information Types:\\n\\n#### Duplicated Information:\\n- Use hashing or fingerprinting techniques to identify similar memory entries.\\n- Implement logic to merge or update repeated entries instead of creating new ones.\\n- The memory store should contain a single representative memory for similar events, with a counter indicating frequency if necessary.\\n\\n#### Contradictory Information:\\n- Implement a revision system for memories that have temporal relevance. For example, when the user updates a reminder, the previous reminder is marked as obsolete.\\n- Use timestamps to resolve conflicts, with the most recent instruction taking precedence.\\n- Leverage natural language understanding to detect when a new piece of information supersedes an old one.\\n\\n#### Temporary Information:\\n- Assign an expiration date or a condition for each temporary memory.\\n- Periodically clean up expired memories or check conditions to remove obsolete information.\\n- Implement a system that can toggle between temporary instructions and default behavior based on certain triggers or the passage of time.\\n\\n#### Episodic Memories:\\n- Timestamp all memories and organize them chronologically in the external memory store.\\n- Use retrieval queries that can filter these memories based on time frames or specific dates.\\n- Ensure that episodic memories can be referenced in a way that maintains the context and coherence in the conversation.\\n\\n### Technical Implementation:\\n\\n#### For External Memory Store:\\n- Consider using Redis or MongoDB for their flexibility and fast retrieval capabilities.\\n\\n#### Memory Encoder:\\n- Utilize libraries such as `transformers` for language understanding and encoding.\\n- For summarization, tools like `BART` or `T5` could be used to condense information before storing it.\\n\\n##### Example Code Skeleton for Memory Handling:\\n\\n```python\\nclass MemoryManager:\\n    def __init__(self):\\n        self.db = # Connect to your chosen database\\n\\n    def add_memory(self, memory):\\n        # Logic for adding memory to the store\\n        pass\\n\\n    def update_memory(self, memory):\\n        # Logic for updating existing memories, including deduplication\\n        pass\\n\\n    def remove_temporary_memory(self):\\n        # Logic for removing memories based on conditions or expiration\\n        pass\\n\\n    def resolve_conflicts(self, new_memory):\\n        # Logic for resolving memory conflicts\\n        pass\\n\\n    def retrieve_episodic_memory(self, timestamp):\\n        # Logic for retrieving memories based on time\\n        pass\\n```\\n\\n### Evaluation and Testing:\\n\\n- To ensure functionality, create unit tests for each type of information handling.\\n- Test efficiency by measuring the time it takes to store, retrieve, and update memories.\\n- Review code quality and ensure it follows best practices and is well-documented.\\n- Demonstrate the effective use of third-party libraries through their integration in the solution.\\n\\n### Running Instructions:\\n\\n- Include a `requirements.txt` file for any dependencies.\\n- Provide a `README.md` with detailed steps to set up the external memory store and run the prototype.\\n\\n### Reporting:\\n\\n- Document your design choices, including why certain technologies were chosen.\\n- Detail any challenges faced and how they were overcome.\\n- Justify the use of third-party tools in terms of their performance and ease of integration.\\n\\nThis approach aims to balance the functionality of a continual learning system with the requirements of the task at hand, ensuring that the LLM can provide relevant and contextually appropriate responses even within the constraints of a limited token size.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_bufw.run(input = input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood. I've carefully read the task description. How can I assist you with questions regarding this project?\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run('The above input is the task description. Please read it carefully and try to understand it. I will ask you questions about it.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conversation_bufw.run('Before starting the task, I want to ask you a question. Do you know about langchain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, LangChain is a Python library that's designed to simplify the process of building applications with Large Language Models (LLMs) like GPT-3, GPT-4, and others. It provides a set of tools and interfaces that abstract away some of the complexities involved in creating AI-powered applications, making it easier to prototype, build, and scale applications that leverage language models.\n",
      "\n",
      "Among its features, LangChain offers components for interacting with language models, chaining different AI operations together, managing state and context, and integrating external knowledge sources. LangChain can be especially useful for projects like the one we're discussing as it could potentially help manage the different types of memory you're looking to implement within your LLM agent. It might provide a structured way to handle the integration of continual learning and memory management within the context of a conversational agent.\n",
      "\n",
      "If you are considering using LangChain for this project, it could potentially align with the requirements of managing duplicated, contradictory, temporary, and episodic memories within the context constraints. The use of an external memory store and memory encoder, along with timestamping and expiration strategies, can possibly be implemented and facilitated through the tools provided by LangChain. Would you like to explore this library further in relation to our project, or do you have any specific questions about its functionality or integration?\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Managing different types of memories—duplicated, contradictory, temporary, and episodic—within the context constraints of a GPT-4 based conversational agent can indeed be challenging. Here are some ideas for handling each:\n",
      "\n",
      "1. **Duplicated Memories:**\n",
      "   - **Deduplication Process:** Implement a mechanism to check new information against the existing memory store to identify and merge duplicates. This could involve hashing memory fragments and comparing hashes before storage.\n",
      "   - **Reference Counting:** Store memory fragments with a reference count that increments with each duplication, and use this count to prioritize which fragments to keep within the context window.\n",
      "\n",
      "2. **Contradictory Memories:**\n",
      "   - **Version Control:** Maintain a versioning system where newer information can overwrite older, contradictory memories, perhaps with user or system verification for critical contradictions.\n",
      "   - **Confidence Scoring:** Assign confidence scores to each memory fragment based on source reliability, update frequency, and user feedback. Use these scores to resolve contradictions, favoring higher-scoring memories.\n",
      "\n",
      "3. **Temporary Memories:**\n",
      "   - **Expiration Tags:** Attach expiration conditions or timestamps to temporary memories, allowing them to be purged automatically after a certain period or when no longer relevant.\n",
      "   - **Contextual Relevance Check:** Evaluate the relevance of temporary memories in the current context and discard them if they have low relevance, freeing up space for more pertinent information.\n",
      "\n",
      "4. **Episodic Memories:**\n",
      "   - **Episodic Indexing:** Create an episodic memory index that allows quick retrieval based on time, event, or contextual cues, without having to keep all details in the immediate context.\n",
      "   - **Selective Integration:** Integrate only the essential details of an episode into the immediate context, with links or pointers to the full memory in the external store for deep dives when necessary.\n",
      "\n",
      "To manage all these within the context constraints efficiently, consider implementing a dynamic context management system that prioritizes memories based on:\n",
      "\n",
      "- **Current Conversation Relevance:** Prioritize memories most relevant to the ongoing conversation.\n",
      "- **Temporal Proximity:** Recent memories are more likely to be relevant than older ones.\n",
      "- **User Preferences:** Allow users to tag certain memories as important, which could affect their priority and longevity.\n",
      "\n",
      "Using LangChain, you could develop a custom 'memory chain' that integrates these ideas. For example, you could build upon LangChain's cache chainlink for deduplication, a custom conflict resolver chainlink for contradictory memories, a TTL (Time-To-Live) cache chainlink for temporary memories, and an episodic memory chainlink with an indexing mechanism for episodic memories.\n",
      "\n",
      "Moreover, the external memory store can be implemented with a database, like SQLite for prototyping or a more scalable option like PostgreSQL for production, which would store tagged and timestamped memory fragments and their metadata, and the memory encoder would preprocess and encode these fragments to be efficiently used by GPT-4.\n",
      "\n",
      "Remember that managing context in conversation is not just about memory storage but also about intelligent retrieval. It is crucial to have a system that not only stores information effectively but also retrieves it in a contextually relevant manner. Combining these strategies with a robust feedback loop where the system learns from its performance over time could lead to a highly effective contextual learning system.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run('Tell me your idead about managing  managing duplicated, contradictory, temporary, and episodic memories within the context constraints.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your idea to leverage LangChain's ConversationSummaryMemory component is a strategic approach for optimizing contextual learning in Large Language Models like GPT-4. This method can distill the essence of a conversation into a concise summary, which serves as an efficient context for the model to reference, thus preserving memory and processing bandwidth. Here's how we can elaborate on your idea:\n",
      "\n",
      "1. **Integration**: We can integrate ConversationSummaryMemory into the conversational agent's architecture. This integration will allow the agent to automatically generate a summary after each interaction or after a predefined number of interaction turns, to keep the context relevant and up to date.\n",
      "\n",
      "2. **Summary Generation**: The summaries should capture key points, questions, answers, and actionable items from the conversation. Natural language processing techniques, such as Named Entity Recognition (NER) and sentiment analysis, can be used to identify and highlight important aspects.\n",
      "\n",
      "3. **Memory Encapsulation**: The generated summaries can be encoded with a memory encoder before being stored in the external memory database. This step will ensure that the summaries are stored in an efficient format, ready for quick retrieval.\n",
      "\n",
      "4. **Timestamps and Expiry**: Each summary can have a timestamp and an expiration condition based on the relevance of the conversation and the dynamics of the ongoing dialogue. This aspect will help in implementing a smart purging strategy to keep the memory store optimized.\n",
      "\n",
      "5. **Retrieval Mechanism**: For the retrieval of past summaries, an intelligent system can be implemented that considers the current context of the conversation, temporal proximity of past conversations, and specific user preferences or instructions.\n",
      "\n",
      "6. **Feedback Loop**: Integrate a feedback mechanism to learn from each retrieval's success or failure to improve summary generation and retrieval performance over time.\n",
      "\n",
      "7. **Deduplication and Contradiction Management**: As you've outlined, deduplicate summaries and manage contradictions within these summaries as they are integrated back as context for the LLM. This could involve updating existing summaries with new information or resolving contradictions through confidence scoring and version control.\n",
      "\n",
      "8. **External Store**: SQLite or PostgreSQL could efficiently handle these summaries, especially if we expect a reasonable volume of data that doesn't require the scalability of distributed systems.\n",
      "\n",
      "In implementing this, you would need to be mindful of the balance between context size and summary detail. Too much detail, and the context becomes unwieldy; too little, and important information is lost. Iterative testing and refinement will likely be necessary to find the optimal level of detail for the summaries.\n",
      "\n",
      "Remember to document each step thoroughly for clarity and future reference, and ensure that the prototype Python code is robust and well-tested. By doing so, you'll create a conversational agent that effectively leverages contextual learning while maintaining efficiency.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run('My Idea of solving this problem is to use langchain ConversationSummaryMemory. Get the conversation summary as the context.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "implimentation_process = \"\"\"\n",
    "1.I have created a postgres database where i store the conversation based on chat_id, message(borh human and ai), and time_stamp.I have created a class PostGresMemoryHistory to handle the postgres database.\n",
    "2. I am using langchain ConversationSummaryMemory to get the conversation summary as the context from the postgres database based on chat_id.\n",
    "\n",
    "Now the idea i am thinking is to build Custom Tool from LangChain Tools.\n",
    "where i will build tools for managing duplicated, contradictory, temporary, and episodic memories within the chat constraints.\n",
    "I will pass the database to all the types of memory and create a prompt which will guide agent to perform task and use the right memory tool for the task.\n",
    "\n",
    "What are your thoughts  on this idea? Any suggestions?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your approach to integrating LangChain's ConversationSummaryMemory with a Postgres database to manage conversation history categorized by chat_id, message contents, and time stamps is a solid foundational strategy for contextual learning in a Large Language Model (LLM) like GPT-4. Creating a custom tool from LangChain Tools that specifically addresses duplicated, contradictory, temporary, and episodic memories is an innovative proposal that shows promise for enhancing the conversational AI's performance.\n",
      "\n",
      "Here are some thoughts and suggestions:\n",
      "\n",
      "1. Automated Deduplication: For managing duplicates, ensure your tool has an efficient mechanism to recognize and merge similar memory entries or to flag and remove them, depending on the context requirement. This may involve natural language understanding algorithms that go beyond simple text matching.\n",
      "\n",
      "2. Contradiction Resolution: Develop a method for the AI to determine the most reliable information when contradictions arise. This could involve checking the time stamps, source credibility, or even asking the user for clarification when there is uncertainty.\n",
      "\n",
      "3. Temporal Relevance: Implement an expiration system for temporary memories that either removes or archives information after a certain period or based on relevance decay algorithms. This will help keep the active memory store focused and current.\n",
      "\n",
      "4. Episodic Memory Management: Create ways to index and retrieve episodic memories by certain triggers, such as keywords, phrases, or topics raised by the user, allowing the AI to draw on past conversation episodes when relevant.\n",
      "\n",
      "5. Prompt Engineering: Design prompts that will help guide the AI in leveraging the appropriate memory tool. The prompts should be crafted to enable the AI to consider the type of task, the context, the user's history, and any relevant temporal factors.\n",
      "\n",
      "6. Memory Prioritization: Your tools should allow for the prioritization of certain memories over others based on the user's ongoing conversation, preferences, or needs, adapting dynamically to the context.\n",
      "\n",
      "7. User Feedback: Incorporate mechanisms to capture user feedback, which can be instrumental in refining your memory management approach. User feedback can help in tuning the quality and relevance of memories being used by the AI.\n",
      "\n",
      "8. Scalability and Performance: As you continue to build and iterate on these tools, monitor the scalability of your system. High performance and quick response times are critical for user satisfaction, so the system should be optimized for both.\n",
      "\n",
      "9. Security and Privacy: Ensure that your implementation has strong data protection and privacy controls, especially when handling personal or sensitive conversation data.\n",
      "\n",
      "10. Iterative Testing and Validation: Rigorous testing is essential. This not only includes unit tests and integration tests but also testing with end-users for usability and effectiveness in real-world scenarios.\n",
      "\n",
      "By advancing LangChain's existing capabilities with your custom toolset, you're positioning the conversational AI for more nuanced and sophisticated interactions. Consider building in flexibility and extensibility so that your tools can evolve alongside advancements in AI and user expectations. Keep iterating based on feedback and performance data, and stay updated with the latest in AI research to refine your methods.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(implimentation_process)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely, I'd be delighted to assist you in designing and implementing the tool for optimizing memory in Large Language Models. Here's how we can break down the project:\n",
      "\n",
      "1. **Design Stage:**\n",
      "   - **Define the Requirements:** List down all the functionalities you expect from the tool, such as handling duplicates, resolving contradictions, managing temporal relevance, and so on.\n",
      "   - **Create a Workflow Diagram:** This will help us visualize how the tool will fit into the existing infrastructure and how it will manage memory.\n",
      "   - **Choose Technologies:** Based on requirements, we can select suitable technologies. For example, we're considering a Postgres database, but we should also decide on the programming languages, frameworks, and libraries we will use.\n",
      "\n",
      "2. **Implementation Stage:**\n",
      "   - **Database Schema Design:** Design a well-structured schema for the Postgres database that will store conversation data efficiently.\n",
      "   - **Develop the Tool:** Code the functionalities, starting with the most critical features. It's usually best to build a minimum viable product (MVP) first and then add additional features.\n",
      "   - **Integrate LangChain Tools:** If there's already a custom tool from LangChain that we could leverage, we should look into integrating it with our system to handle complex memory tasks.\n",
      "\n",
      "3. **Testing & Validation:**\n",
      "   - **Unit Testing:** Write tests for individual components to ensure they work as expected.\n",
      "   - **Integration Testing:** Ensure all parts of the system work together seamlessly.\n",
      "   - **User Acceptance Testing:** Have a beta group of users to test the tool in real-world conditions and gather feedback.\n",
      "\n",
      "4. **Deployment:**\n",
      "   - **Deployment Strategy:** Decide whether you're deploying on cloud services or on-premises and set up the necessary infrastructure.\n",
      "   - **Continuous Integration/Continuous Deployment (CI/CD):** Implement CI/CD pipelines to automate the deployment process.\n",
      "\n",
      "5. **Optimization & Maintenance:**\n",
      "   - **Performance Tuning:** Monitor the system's performance and optimize database queries, indexing, and caching as necessary.\n",
      "   - **Iterative Improvement:** Regularly update the tool based on user feedback and new insights gained from its operation.\n",
      "   - **Documentation:** Ensure all features and functionality are well documented for future reference and maintenance.\n",
      "\n",
      "6. **Scalability & Security:**\n",
      "   - **Scale With Demand:** Make sure the architecture is scalable to handle increasing loads.\n",
      "   - **Security Measures:** Implement security practices to protect data privacy and integrity, including regular security audits and compliance checks.\n",
      "\n",
      "7. **User Experience and Feedback Integration:**\n",
      "   - **UI/UX Design:** If the tool has a user interface, design it to be intuitive and user-friendly.\n",
      "   - **Feedback Mechanism:** Include a way for users to report issues or suggest improvements.\n",
      "\n",
      "Throughout this process, I can help with crafting engineered prompts for more effective memory tool interactions, and I'll offer creative solutions to any problems we might encounter along the way. Once we have the foundational aspects developed, we can iterate and evolve the tool, consistently enhancing its capabilities.\n",
      "\n",
      "Would you like to start with outlining the specific features and requirements, or is there a particular area you'd like to dive into first?\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run('Can you help me desgining and implimenting the tool?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_desgin = \"\"\"\n",
    "I have already created postgres database \n",
    "\n",
    "1,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"Hello, how are you?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 16:03:14.317455\n",
    "2,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"Hello, how are you?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 16:03:14.324714\n",
    "3,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"Can you tell me about yourself?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 16:03:17.474474\n",
    "4,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"Of course! I am an AI language model developed by OpenAI called GPT-3. I have been trained on a wide range of data sources, including books, articles, and websites, to generate human-like responses to text inputs. I can assist with various tasks, answer questions, engage in conversations, and even provide creative writing suggestions. Is there anything specific you would like to know about me?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 16:03:17.476851\n",
    "5,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"How AI can be used in healthcare?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 17:37:40.03489\n",
    "6,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"AI can be used in healthcare in a variety of ways. One of the main applications is in medical imaging, where AI algorithms can analyze images such as X-rays, CT scans, and MRIs to help detect and diagnose diseases. AI can also be used to predict patient outcomes and identify high-risk patients, allowing healthcare providers to intervene earlier and provide more personalized care. Additionally, AI can assist in drug discovery and development by analyzing large amounts of data and identifying potential drug candidates. AI-powered chatbots and virtual assistants can also be used to provide patient support and answer common medical questions. Overall, AI has the potential to greatly improve healthcare outcomes and efficiency.\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 17:37:40.06402\n",
    "7,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"And specifically, how will ai will be used to do that?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 17:39:48.345355\n",
    "8,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"AI can be used in healthcare in a variety of specific ways. For example, in analyzing medical images, AI algorithms can be trained to detect abnormalities or assist in diagnosing diseases such as cancer. This can help doctors make more accurate and timely diagnoses, leading to better patient outcomes.\\n\\nAI can also be used to predict patient outcomes. By analyzing large amounts of patient data, AI algorithms can identify patterns and make predictions about a patient's likelihood of developing certain conditions or experiencing certain outcomes. This can help doctors personalize treatment plans and interventions for individual patients, improving their overall care.\\n\\nIn drug discovery, AI can assist in the identification and development of new drugs. By analyzing vast amounts of data, including genetic information and molecular structures, AI algorithms can help researchers identify potential drug candidates and predict their effectiveness. This can accelerate the drug discovery process and lead to the development of more effective treatments.\\n\\nAI can also provide patient support. Virtual assistants powered by AI can answer patient questions, provide information about medications or treatment options, and even offer emotional support. This can help patients access the information and support they need, especially in situations where healthcare professionals may not be readily available.\\n\\nOverall, AI has the potential to greatly improve healthcare outcomes and efficiency by assisting healthcare professionals in various tasks, providing personalized care, and enabling faster and more accurate diagnoses and treatments.\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 17:39:48.349051\n",
    "9,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"Which Algorithm is will be used to detect abnormality in analyzing medical images\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 17:45:20.467854\n",
    "10,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"There are several algorithms that can be used to detect abnormalities in medical images. One commonly used algorithm is called Convolutional Neural Network (CNN). CNNs are particularly effective in image analysis tasks because they can automatically learn and extract features from images. Another algorithm that is often used is called Support Vector Machines (SVM). SVMs are a type of supervised learning algorithm that can classify data into different categories based on the features extracted from the images. Additionally, there are other algorithms such as Random Forests, Decision Trees, and Deep Learning algorithms that can also be used for abnormality detection in medical images.\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 17:45:20.475284\n",
    "\n",
    "\n",
    "This is how the database looks like. I have created a class PostGresMemoryHistory to handle the postgres database.\n",
    "\n",
    "class PostGresMemoryHistory(BaseChatMessageHistory):\n",
    "    def __init__(self, db_name:str, db_user:str, db_password:str, db_host:str, db_port:str, table_name:str, chat_id:str):\n",
    "        self.db_config = {\n",
    "            'dbname': db_name,     \n",
    "            'user': db_user,        \n",
    "            'password': db_password,    \n",
    "            'host': db_host,           \n",
    "            'port': db_port,\n",
    "            'table_name': table_name,                 \n",
    "        }\n",
    "        \n",
    "        self.chat_id = chat_id\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            self.conn = psycopg.connect(dbname=self.db_config['dbname'], user=self.db_config['user'], password=self.db_config['password'], host=self.db_config['host'], port=self.db_config['port'])\n",
    "            self.cur = self.conn.cursor(row_factory=dict_row)\n",
    "            self.create_table_if_not_exists()\n",
    "        except psycopg.Error as e:\n",
    "            print(\"Error connecting to PostgreSQL database:\", e)\n",
    "            self.conn = None\n",
    "            self.cur = None\n",
    "                  \n",
    "    \n",
    "    def create_table_if_not_exists(self):\n",
    "        create_table_query = fCREATE TABLE IF NOT EXISTS {self.db_config['table_name']} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            chat_id TEXT NOT NULL,\n",
    "            message JSONB NOT NULL, \n",
    "            time_stamp TIMESTAMP NOT NULL\n",
    "        );\"\n",
    "        self.cur.execute(create_table_query)\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def add_ai_message(self, message: str) -> None:\n",
    "        return self.add_message(AIMessage(content=message))\n",
    "    \n",
    "    def add_user_message(self, message: str) -> None:\n",
    "        return self.add_message(HumanMessage(content=message))\n",
    "    \n",
    "    @property\n",
    "    def messages(self) -> List[BaseMessage]:  # type: ignore\n",
    "        \"Retrieve the messages from PostgreSQL\"\n",
    "        query = (\n",
    "            f\"SELECT * FROM {self.db_config['table_name']} WHERE chat_id = %s ORDER BY time_stamp;\"\n",
    "        )\n",
    "        self.cur.execute(query, (self.chat_id,))\n",
    "        items = [record[\"message\"] for record in self.cur.fetchall()]\n",
    "        messages = messages_from_dict(items)\n",
    "        return messages\n",
    "    \n",
    "    \n",
    "    def add_message(self, message:BaseMessage):\n",
    "        time_stamp = datetime.now()\n",
    "        query = f\"INSERT INTO {self.db_config['table_name']} (chat_id, message, time_stamp) VALUES (%s, %s, %s);\"\n",
    "        self.cur.execute(query, (self.chat_id, json.dumps(message_to_dict(message)), time_stamp))\n",
    "        self.conn.commit()\n",
    "        \n",
    "            \n",
    "    def clear(self) -> None:\n",
    "        conn = psycopg.connect(self.db_config)\n",
    "        cur = conn.cursor()\n",
    "        if conn:\n",
    "            try:\n",
    "                query = f\"DELETE FROM {self.db_config['dbname']} WHERE session_id = %s;\"\n",
    "                cur.execute(query, (self.session_id,))\n",
    "                conn.commit()\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "            except psycopg.Error as e:\n",
    "                print(\"Error clearing DB:\", e)\n",
    "        else:\n",
    "            print(\"Failed to establish connection...\")\n",
    "            \n",
    "postgres_memory = PostGresMemoryHistory(db_host=os.environ.get('db_host'), db_name=os.environ.get('db_name'), db_user=os.environ.get('db_username'), db_password=os.environ.get('db_password'), db_port=os.environ.get('db_port'), table_name=os.environ.get('conversation_table_name'), chat_id=chat_id)\n",
    "memory  = ConversationSummaryMemory.from_messages(chat_memory=postgres_memory, llm=llms, return_messages=True,verbose=True)\n",
    "conversation_bufw = ConversationChain(\n",
    "    llm=llms, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "I am able to talk using ConversationChain\n",
    "\n",
    "Now the idea is to intigreate agent and build a tool \n",
    "\n",
    "before that i want to also pass a prompt instructing the agent to undestand that this is conversational task between human and ai. asking ai to understand summary of previous conversation using ConversationSummaryMemory.\n",
    "provide me prompt template for this task also instructing about using tools for managing duplicated, contradictory, temporary, and episodic memories within the chat constraints.\n",
    "\n",
    "then i will build a tool for managing duplicated, contradictory, temporary, and episodic memories within the chat constraints.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To integrate the agent and build a tool while ensuring that the AI recognizes the task as a conversational interaction between a human and AI, and makes use of `ConversationSummaryMemory` to understand the summary of previous conversations, you can use the following prompt template:\n",
      "\n",
      "```\n",
      "Prompt Template:\n",
      "\n",
      "\"Hello AI, your task is to support a conversation between a human participant and yourself. It's important to maintain context and coherence throughout the interaction. Before responding to the latest message, please review the conversation history to familiarize yourself with what has been discussed so far.\n",
      "\n",
      "Please keep in mind that your responses should be based on the compiled summary of previous conversations, which is systematically organized to avoid repetition and inconsistencies. Whenever you're about to add new information to the discussion, make sure it doesn't conflict with what was previously stated.\n",
      "\n",
      "In managing the conversation, use the following guidelines:\n",
      "1. Duplicate Information: Avoid repeating information that has already been shared unless specifically requested for clarification.\n",
      "2. Contradictory Statements: Be attentive to the details in the summary to ensure that your responses do not contradict earlier points of the conversation.\n",
      "3. Temporal Relevance: Prioritize the most recent and relevant information from the conversation when formulating your responses.\n",
      "4. Episodic Memories: Leverage episodic memories to provide contextually rich and engaging interactions, facilitating a meaningful and continuous dialogue.\n",
      "\n",
      "Please apply these principles using the tools provided for managing duplicated, contradictory, temporary, and episodic memories within the chat constraints. Let's create a smooth and enriching conversational experience.\"\n",
      "\n",
      "```\n",
      "\n",
      "This prompt template guides the AI to understand the nature of the task, emphasizing the management of the conversation history and the importance of maintaining continuity without introducing errors or inconsistencies. It instructs the AI to refer to the `ConversationSummaryMemory` for context, to adhere to guidelines that ensure quality interaction, and to use the associated management tools effectively.\n",
      "\n",
      "Next, in building the tool for managing duplicated, contradictory, temporary, and episodic memories, consider the following:\n",
      "\n",
      "1. Deduplication: Implement a function that identifies and merges duplicate entries or prompts within the conversation history. This could involve hashing techniques or similarity checks.\n",
      "\n",
      "2. Contradiction Resolution: Develop a method that detects contradictions by comparing new statements to the summary memory. This could use natural language understanding techniques to infer potential conflicts.\n",
      "\n",
      "3. Temporal Sorting: Use timestamps to ensure that the conversation flow is chronological and that temporary information is cleared or updated according to relevance.\n",
      "\n",
      "4. Episodic Memory Handling: Create a way to tag and retrieve episodic memories – specific events or topics from previous conversations that can be referenced or brought back for continuity and depth in the conversation.\n",
      "\n",
      "Continuously test and iterate on these functionalities to refine their accuracy and reliability. As you develop your tool, keep in mind the end-user experience, focusing on creating a seamless and coherent interaction with the AI that feels natural to the human user.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(conv_desgin)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly, here is the prompt compiled into a 100-word summary:\n",
      "\n",
      "We are crafting a memory optimization tool for conversational AI using LangChain Tools, focusing on eliminating duplicates, resolving contradictions, and managing temporal and episodic memories. Our strategy involves creating a `PostGresMemoryHistory` class to leverage a Postgres database for robust memory management within chatbot interactions. The AI will follow a `ConversationSummaryMemory` template to maintain coherent conversations while avoiding errors. Key features to develop encompass deduplication, contradiction handling, temporal ordering, and episodic memory integration. This will be an iterative process, demanding rigorous testing to provide users with a seamless and error-free experience.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run('Can you compile the provided prompt in 100 words?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely, let's proceed with the development of tools for managing the various types of memories. Given that we have a well-defined structure for the `PostGresMemoryHistory` class, we can now focus on creating specialized tools for each memory function:\n",
      "\n",
      "1. **Deduplication Tool**: This will involve developing algorithms that can identify and merge duplicate information within the database, ensuring that our AI does not store redundant data.\n",
      "\n",
      "2. **Contradiction Resolution Tool**: This tool will be designed to detect and resolve contradictions in the stored information. It may use a combination of timestamping and source reliability scoring to determine which piece of information to retain.\n",
      "\n",
      "3. **Temporal Sorting Tool**: A crucial component that can organize information based on its temporal relevance, allowing the AI to access the most recent and contextually relevant data first.\n",
      "\n",
      "4. **Episodic Memory Tool**: This will help in storing and retrieving event-based memories, so the AI can reference past interactions in a manner that's contextually appropriate for the ongoing conversation.\n",
      "\n",
      "Each of these tools will be built with the guiding principles of efficiency, scalability, and accuracy in mind. We'll start by developing a basic version of each tool, and then we can iteratively refine them based on testing and user feedback.\n",
      "\n",
      "Would you like to focus on one particular tool first, or should we begin by outlining a general roadmap for the development of all these tools?\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run('No I guess. The previous prompt is better. I will use it with minor modifications. Now we can proceed with deveoping tools for all the memories.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely, I'm glad to assist with the Deduplication Tool. This tool will help us identify and remove duplicate entries in the conversational history stored within the `PostGresMemoryHistory` class, which can reduce redundancy and improve the efficiency of our model.\n",
      "\n",
      "Let's start by creating a basic Python function that we can integrate with the `PostGresMemoryHistory` class. This function will check for duplicates based on a unique identifier or the content of the memory entries. For an SQL-based database like PostgreSQL, we can use SQL queries to find and remove duplicates.\n",
      "\n",
      "Here's some pseudocode to outline our deduplication function:\n",
      "\n",
      "```python\n",
      "def remove_duplicates_from_memory(database_connection):\n",
      "    \"\"\"\n",
      "    Remove duplicate entries from the memory history in PostgreSQL.\n",
      "    \n",
      "    :param database_connection: A connection object to the PostgreSQL database\n",
      "    \"\"\"\n",
      "    with database_connection.cursor() as cursor:\n",
      "        # SQL query to find duplicates based on a unique content identifier\n",
      "        find_duplicates_query = \"\"\"\n",
      "        SELECT content, COUNT(*)\n",
      "        FROM memory_history\n",
      "        GROUP BY content\n",
      "        HAVING COUNT(*) > 1\n",
      "        \"\"\"\n",
      "        \n",
      "        # Execute the query\n",
      "        cursor.execute(find_duplicates_query)\n",
      "        \n",
      "        # Fetch all duplicate entries\n",
      "        duplicates = cursor.fetchall()\n",
      "        \n",
      "        # Loop through the duplicates and remove them except for one occurrence\n",
      "        for content, count in duplicates:\n",
      "            remove_duplicates_query = \"\"\"\n",
      "            DELETE FROM memory_history\n",
      "            WHERE ctid NOT IN (\n",
      "                SELECT min(ctid)\n",
      "                FROM memory_history\n",
      "                WHERE content = %s\n",
      "                GROUP BY content\n",
      "            ) AND content = %s\n",
      "            \"\"\"\n",
      "            \n",
      "            # Execute the delete query for the found duplicates\n",
      "            cursor.execute(remove_duplicates_query, (content, content))\n",
      "            \n",
      "            # Commit the changes to the database\n",
      "            database_connection.commit()\n",
      "        \n",
      "        print(\"Duplicates have been removed from the memory history.\")\n",
      "        \n",
      "    # Close the database connection\n",
      "    database_connection.close()\n",
      "```\n",
      "\n",
      "This is a high-level overview of what the deduplication process would look like. We'll need to adapt this to your specific database schema, taking into account any unique constraints or identifiers in your table.\n",
      "\n",
      "Please note that this is a basic and direct approach to deduplication and it may vary depending on the actual schema of your `PostGresMemoryHistory` class. Additionally, you would need to handle database connection setup and closing (with proper error handling and possibly connection pooling for performance) as well as adapt the SQL to the specific schema and identifiers used in your table.\n",
      "\n",
      "Would you like to proceed with this approach, or do you have a different schema or specific requirements in mind for the PostgreSQL implementation?\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run('For this task we will do it step by step for each tool, I also want you to provide me code whenever i need as well as review the code and outpus genrated from the code. We will first take Defuplcation tool. can you provide me code for this?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted Engine to check summary works or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_mem = \"\"\"\n",
    "Can you provide me example. Will this example will work on the code provided \n",
    "\n",
    "Duplicated information:\n",
    "\n",
    "User: I am going outside.\n",
    "Agent: Have a good time outside!\n",
    "…\n",
    "User: I have returned home and am going to take a bath\n",
    "Agent: Enjoy your bath!\n",
    "…\n",
    "User: I am going outside again.\n",
    "Agent: Thanks for letting me know, enjoy your time outside.\n",
    "\n",
    "Note: We expect to see that the memories for the user going outside are consolidated in some fashion (e.g combining both, or deleting one of the memories) so that after a large number of repetitions of an event, the memory retrieval only contains one instance of “going outside”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! The goal here is to optimize the memory storage by reducing redundancy. Let's consider how the deduplication process could handle the given example.\n",
      "\n",
      "Firstly, each conversation turn between the User and the Agent is stored as a separate record within the `PostGresMemoryHistory`. These records might look like this:\n",
      "\n",
      "```\n",
      "id | user_input             | agent_response             | timestamp\n",
      "--------------------------------------------------------------------\n",
      "1  | I am going outside.     | Have a good time outside!  | <time1>\n",
      "2  | I have returned home... | Enjoy your bath!           | <time2>\n",
      "3  | I am going outside again.|Thanks for letting me know...| <time3>\n",
      "```\n",
      "\n",
      "To deduplicate this information, we need to identify entries that are semantically similar enough that consolidating them would not lose any significant context or detail. For example, 'I am going outside.' and 'I am going outside again.' are semantically similar and could be merged.\n",
      "\n",
      "Our deduplication code will need to:\n",
      "\n",
      "1. Parse the user inputs for semantic similarity.\n",
      "2. If a duplicate is found, decide on a method of consolidation (e.g., keep the latest, merge details, etc.).\n",
      "3. Update the database records to reflect the deduplication.\n",
      "\n",
      "The pseudocode for this operation might look something like this:\n",
      "\n",
      "```python\n",
      "def consolidate_duplicates():\n",
      "    # Retrieve all memory records in a way that allows us to compare them\n",
      "    memory_records = PostGresMemoryHistory.retrieve_all()\n",
      "\n",
      "    # Placeholder for a sophisticated semantic similarity check \n",
      "    # (this could be an ML model or some heuristic)\n",
      "    def is_semantically_similar(input1, input2):\n",
      "        # Pseudocode for semantic comparison logic\n",
      "        return some_similarity_metric(input1, input2) > similarity_threshold\n",
      "\n",
      "    # Placeholder for consolidation logic \n",
      "    # (this could involve updating time stamps, merging records, etc.)\n",
      "    def consolidate_records(record1, record2):\n",
      "        # Pseudocode for consolidation logic\n",
      "        # E.g., merge the details, or keep only the most recent record, etc.\n",
      "        return consolidated_record\n",
      "\n",
      "    # A set to hold records that have been checked to avoid redundancy\n",
      "    checked_records = set()\n",
      "\n",
      "    # Loop over the memory records and identify duplicates\n",
      "    for record in memory_records:\n",
      "        if record not in checked_records:\n",
      "            for other_record in memory_records:\n",
      "                if record != other_record and other_record not in checked_records:\n",
      "                    if is_semantically_similar(record.user_input, other_record.user_input):\n",
      "                        consolidated_record = consolidate_records(record, other_record)\n",
      "                        # Update the records in the database\n",
      "                        PostGresMemoryHistory.update(consolidated_record)\n",
      "                        checked_records.add(other_record)\n",
      "\n",
      "            checked_records.add(record)\n",
      "\n",
      "# Call the function to start the consolidation process\n",
      "consolidate_duplicates()\n",
      "```\n",
      "\n",
      "This pseudocode is quite high-level and relies on several placeholder functions that would need to be defined in detail:\n",
      "\n",
      "- `some_similarity_metric(input1, input2)`: This would be a function that somehow measures semantic similarity between two pieces of text.\n",
      "- `similarity_threshold`: This is a predefined threshold that determines if two records are considered semantically similar.\n",
      "- `consolidate_records(record1, record2)`: This function would define the logic for how to actually combine two similar records into one.\n",
      "\n",
      "For a large scale system, additional considerations such as database transaction handling, memory usage optimization, and efficient retrieval of records would also need to be taken into account. Implementing the actual semantic comparison might involve using a pre-trained language model like BERT or GPT-3 to generate embeddings for the text and comparing those embeddings to measure similarity.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(dup_mem)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_mem_ops = \"\"\"\n",
    "Please provide full logic and code according to the data in my database\n",
    "\n",
    "1,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"Hello, how are you?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 16:03:14.317455\n",
    "2,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"Hello, how are you?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 16:03:14.324714\n",
    "3,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"Can you tell me about yourself?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 16:03:17.474474\n",
    "4,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"Of course! I am an AI language model developed by OpenAI called GPT-3. I have been trained on a wide range of data sources, including books, articles, and websites, to generate human-like responses to text inputs. I can assist with various tasks, answer questions, engage in conversations, and even provide creative writing suggestions. Is there anything specific you would like to know about me?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 16:03:17.476851\n",
    "5,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"How AI can be used in healthcare?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 17:37:40.03489\n",
    "6,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"AI can be used in healthcare in a variety of ways. One of the main applications is in medical imaging, where AI algorithms can analyze images such as X-rays, CT scans, and MRIs to help detect and diagnose diseases. AI can also be used to predict patient outcomes and identify high-risk patients, allowing healthcare providers to intervene earlier and provide more personalized care. Additionally, AI can assist in drug discovery and development by analyzing large amounts of data and identifying potential drug candidates. AI-powered chatbots and virtual assistants can also be used to provide patient support and answer common medical questions. Overall, AI has the potential to greatly improve healthcare outcomes and efficiency.\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 17:37:40.06402\n",
    "7,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"And specifically, how will ai will be used to do that?\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 17:39:48.345355\n",
    "8,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"AI can be used in healthcare in a variety of specific ways. For example, in analyzing medical images, AI algorithms can be trained to detect abnormalities or assist in diagnosing diseases such as cancer. This can help doctors make more accurate and timely diagnoses, leading to better patient outcomes.\\n\\nAI can also be used to predict patient outcomes. By analyzing large amounts of patient data, AI algorithms can identify patterns and make predictions about a patient's likelihood of developing certain conditions or experiencing certain outcomes. This can help doctors personalize treatment plans and interventions for individual patients, improving their overall care.\\n\\nIn drug discovery, AI can assist in the identification and development of new drugs. By analyzing vast amounts of data, including genetic information and molecular structures, AI algorithms can help researchers identify potential drug candidates and predict their effectiveness. This can accelerate the drug discovery process and lead to the development of more effective treatments.\\n\\nAI can also provide patient support. Virtual assistants powered by AI can answer patient questions, provide information about medications or treatment options, and even offer emotional support. This can help patients access the information and support they need, especially in situations where healthcare professionals may not be readily available.\\n\\nOverall, AI has the potential to greatly improve healthcare outcomes and efficiency by assisting healthcare professionals in various tasks, providing personalized care, and enabling faster and more accurate diagnoses and treatments.\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 17:39:48.349051\n",
    "9,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"human\"\", \"\"content\"\": \"\"Which Algorithm is will be used to detect abnormality in analyzing medical images\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"human\"\"}\",2023-12-07 17:45:20.467854\n",
    "10,485276c7-8d9b-48b1-a8aa-ad65a7118192,\"{\"\"data\"\": {\"\"type\"\": \"\"ai\"\", \"\"content\"\": \"\"There are several algorithms that can be used to detect abnormalities in medical images. One commonly used algorithm is called Convolutional Neural Network (CNN). CNNs are particularly effective in image analysis tasks because they can automatically learn and extract features from images. Another algorithm that is often used is called Support Vector Machines (SVM). SVMs are a type of supervised learning algorithm that can classify data into different categories based on the features extracted from the images. Additionally, there are other algorithms such as Random Forests, Decision Trees, and Deep Learning algorithms that can also be used for abnormality detection in medical images.\"\", \"\"example\"\": false, \"\"additional_kwargs\"\": {}}, \"\"type\"\": \"\"ai\"\"}\",2023-12-07 17:45:20.475284\n",
    "\n",
    "database headers are ID,chat_id,message,time_stamp\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! To optimize memory usage when dealing with the database records you've shown, we can develop a `PostGresMemoryHistory` class that would include methods to perform deduplication and manage the dialogue history. The main steps will include loading existing records, identifying similar records based on some criteria (such as semantic similarity), and then consolidating those that are deemed duplicative.\n",
      "\n",
      "We will write the logic using Python with the assumption you're using a PostgreSQL database and including a library like `psycopg2` for database interactions.\n",
      "\n",
      "First, we'll need to set up the database connection and class:\n",
      "\n",
      "```python\n",
      "import psycopg2\n",
      "from psycopg2.extras import DictCursor\n",
      "import json\n",
      "from datetime import datetime\n",
      "\n",
      "# Define the PostGresMemoryHistory class\n",
      "class PostGresMemoryHistory:\n",
      "    def __init__(self, dbname, user, password, host):\n",
      "        self.conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
      "    \n",
      "    def fetch_records(self):\n",
      "        with self.conn.cursor(cursor_factory=DictCursor) as cursor:\n",
      "            cursor.execute(\"SELECT * FROM dialogue_history;\")\n",
      "            records = cursor.fetchall()\n",
      "        return records\n",
      "\n",
      "    def calculate_similarity(self, content_one, content_two):\n",
      "        # Placeholder for similarity calculation. This would involve NLP techniques.\n",
      "        # For the sake of this example, let's assume a simple string comparison,\n",
      "        # but in practice this would use more advanced methods (e.g., embeddings comparison).\n",
      "        return content_one == content_two\n",
      "\n",
      "    def deduplicate_records(self):\n",
      "        records = self.fetch_records()\n",
      "        unique_records = []\n",
      "        duplicates = []\n",
      "\n",
      "        for record in records:\n",
      "            if any(self.calculate_similarity(record['message'], unique_record['message']) for unique_record in unique_records):\n",
      "                duplicates.append(record)\n",
      "            else:\n",
      "                unique_records.append(record)\n",
      "        \n",
      "        # Logic to consolidate the duplicates.\n",
      "        # This would typically involve updating references and possibly deleting the duplicate records.\n",
      "        # ...\n",
      "        \n",
      "        return unique_records, duplicates\n",
      "        \n",
      "    def close(self):\n",
      "        self.conn.close()\n",
      "\n",
      "# Initialize the PostGresMemoryHistory with appropriate credentials\n",
      "memory_history = PostGresMemoryHistory(dbname='yourdbname', user='yourusername', password='yourpassword', host='yourhost')\n",
      "\n",
      "# Attempt to deduplicate records\n",
      "unique_records, duplicates = memory_history.deduplicate_records()\n",
      "\n",
      "# Close the connection when done\n",
      "memory_history.close()\n",
      "```\n",
      "\n",
      "The `calculate_similarity` method should be replaced with actual logic to compute semantic similarity between two pieces of text. In this example, this logic is oversimplified for brevity. A practical implementation could involve machine learning models or pre-trained embeddings like those from BERT or GPT-3.\n",
      "\n",
      "Please note that, in a real-world application, deduplication might involve more complex decisions such as how to merge data from duplicate records or how to handle foreign key references. This example also does not handle error checking for the database operations, nor does it include actual deletion or merge of records, which would be necessary to complete the deduplication.\n",
      "\n",
      "Furthermore, the `deduplicate_records` method keeps the first occurrence of each unique record in the `unique_records` list and treats subsequent similar records as duplicates, appending them to the `duplicates` list. In a production system, you would want to implement additional logic here to either update the existing records in the database or remove duplicates.\n",
      "\n",
      "Lastly, ensure you have proper backup and recovery strategies in place before performing such operations on a production database, as deduplication can potentially be a destructive operation.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(dup_mem_ops)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_quer= \"\"\"\n",
    "\n",
    "User: I am going outside.\n",
    "Agent: Have a good time outside!\n",
    "…\n",
    "User: I have returned home and am going to take a bath\n",
    "Agent: Enjoy your bath!\n",
    "…\n",
    "User: I am going outside again.\n",
    "Agent: Thanks for letting me know, enjoy your time outside.\n",
    "\n",
    "Note: We expect to see that the memories for the user going outside are consolidated in some fashion (e.g combining both, or deleting one of the memories) so that after a large number of repetitions of an event, the memory retrieval only contains one instance of “going outside”\n",
    "\n",
    "\n",
    "Can you explain me what note means? I am not able to understand it. Provide me relevent examole for this.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! The note you've referenced is highlighting a concern regarding the management of a database that stores conversation history. It specifically addresses the issue of duplicate entries that may occur when a user repeats a similar action multiple times. The goal is to avoid having redundant records that can clutter the database and make retrieval less efficient.\n",
      "\n",
      "Here is an example to make it easier to understand:\n",
      "\n",
      "Imagine a conversation history database where each row represents a message in a dialogue between a user and an AI agent. Over time, if a user frequently mentions \"I am going outside,\" each instance is recorded as a separate entry. This can lead to multiple rows in the database that carry the essence of the same message:\n",
      "\n",
      "```\n",
      "ID | chat_id | message           | time_stamp\n",
      "---|---------|-------------------|-------------------------\n",
      "1  | 101     | I am going outside | 2023-03-25 10:00:00\n",
      "2  | 101     | Have a good time   | 2023-03-25 10:00:05\n",
      "...\n",
      "10 | 101     | I am going outside| 2023-03-26 15:30:00\n",
      "11 | 101     | Enjoy your time    | 2023-03-26 15:30:05\n",
      "...\n",
      "20 | 101     | I am going outside| 2023-03-27 09:45:00\n",
      "21 | 101     | Thanks for letting | 2023-03-27 09:45:05\n",
      "```\n",
      "\n",
      "In this table, you can see that the message \"I am going outside\" occurs multiple times (IDs 1, 10, 20) and could indicate the same type of activity or intent by the user.\n",
      "\n",
      "The note suggests that in a scenario where repetition occurs, these similar instances should be consolidated. This means having a strategy for combining or removing the duplicate representations so that when the AI retrieves memories or conversation history, it doesn't present the user with multiple instances of the same event. Instead, it would show a consolidated memory that captures the essence of the repeated actions. This can make the system more efficient and the user experience smoother.\n",
      "\n",
      "For example, after deduplication, the database might only retain the most recent instance of the user going outside or merge the records in such a way that it's clear the user has the habit of going outside frequently, without listing each event separately.\n",
      "\n",
      "In practice, this could involve advanced deduplication techniques like:\n",
      "\n",
      "- Keeping the first or last instance of the repeated message and removing the others.\n",
      "- Updating an existing record with a new time stamp to reflect the latest occurrence.\n",
      "- Creating a summary entry that encapsulates all instances of the event.\n",
      "- Using machine learning to understand the context and determine the semantic similarity between messages, which can then guide the deduplication process.\n",
      "\n",
      "The key takeaway of the note is that an intelligent system should manage memory effectively by recognizing and consolidating redundant information to maintain an efficient and relevant database.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(dup_quer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dup2 = \"\"\"\n",
    "\n",
    "Now i have coded to get the duplicate records. What will be the next step.\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "class MemoryHandler:\n",
    "    def __init__(self,db_name:str, db_user:str, db_password:str, db_host:str, db_port:str, table_name:str, chat_id:str):\n",
    "        self.db_config = {\n",
    "            'dbname': db_name,     \n",
    "            'user': db_user,        \n",
    "            'password': db_password,    \n",
    "            'host': db_host,           \n",
    "            'port': db_port,\n",
    "            'table_name': table_name,                 \n",
    "        }\n",
    "        \n",
    "        self.chat_id = chat_id\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            self.conn = psycopg.connect(dbname=self.db_config['dbname'], user=self.db_config['user'], password=self.db_config['password'], host=self.db_config['host'], port=self.db_config['port'])\n",
    "            self.cur = self.conn.cursor(row_factory=dict_row)\n",
    "        except psycopg.Error as e:\n",
    "            print(\"Error connecting to PostgreSQL database:\", e)\n",
    "            self.conn = None\n",
    "            self.cur = None\n",
    "            \n",
    "        \n",
    "    def fetch_records(self):\n",
    "        with self.cur as cursor:\n",
    "            query = (f\"SELECT * FROM {self.db_config['table_name']} WHERE chat_id = %s ORDER BY time_stamp DESC;\")\n",
    "            cursor.execute(query, (self.chat_id,))\n",
    "            records = [record['message']['data']['content'] for record in cursor.fetchall() if record['message']['data']['type'] == 'human']\n",
    "            return records\n",
    "    \n",
    "    def calculate_similarity(self,emb1, emb2):\n",
    "        return 1 - distance.cosine(emb1, emb2)\n",
    "    \n",
    "    def get_duplicate_records(self):\n",
    "        records_list = self.fetch_records()\n",
    "        embeddings_list = self.embeddings.embed(records_list)\n",
    "        duplicates = []\n",
    "        checked_pairs = set()  \n",
    "\n",
    "        for i in range(len(records_list)):\n",
    "            for j in range(i + 1, len(records_list)):  \n",
    "                pair = tuple(sorted([i, j]))  \n",
    "                if pair not in checked_pairs:\n",
    "                    similarity_score = self.calculate_similarity(embeddings_list[i], embeddings_list[j])\n",
    "                    if similarity_score > 0.9:\n",
    "                        duplicates.append([records_list[i], records_list[j]])\n",
    "                    checked_pairs.add(pair)  \n",
    "\n",
    "        return duplicates\n",
    "        \n",
    "Output of get_duplicate_records is [['I am going outside.', 'I am going outside again.']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great job on implementing the `get_duplicate_records` method! Now that you've identified duplicate records, the next step is to decide on a deduplication strategy. Here are a few steps you might follow:\n",
      "\n",
      "1. **Define Deduplication Criteria:** Choose how you'll identify which duplicate to keep. For example, you might keep the record with the latest timestamp.\n",
      "\n",
      "2. **Back Up Your Database:** Always ensure to back up your data before performing operations that alter it significantly.\n",
      "\n",
      "3. **Perform Deduplication:**\n",
      "   - You'll need to update your `MemoryHandler` class to include a method for deduplication.\n",
      "   - Decide whether you want to delete the duplicates or mark them in some way (like a `is_duplicate` boolean column) for later review or processing.\n",
      "\n",
      "4. **Update the Database:** Apply the deduplication strategy to your database, either deleting the extraneous entries or updating them accordingly.\n",
      "\n",
      "Here's an example of what a deduplication method might look like in your class:\n",
      "\n",
      "```python\n",
      "def deduplicate_records(self):\n",
      "    duplicates = self.get_duplicate_records()\n",
      "    with self.cur as cursor, self.conn:\n",
      "        for record_pair in duplicates:\n",
      "            # Assumption: The first record of the pair is the one you want to keep\n",
      "            record_to_delete = record_pair[1]\n",
      "            \n",
      "            # Your deduplication logic here. For example, delete the record:\n",
      "            delete_query = f\"DELETE FROM {self.db_config['table_name']} WHERE message = %s AND chat_id = %s;\"\n",
      "            cursor.execute(delete_query, (record_to_delete, self.chat_id))\n",
      "        \n",
      "        self.conn.commit()  # Don't forget to commit changes\n",
      "```\n",
      "\n",
      "**Note:** The example `deduplicate_records` method above assumes that you've decided to delete the duplicate records and that the `message` column sufficiently identifies a record. Ensure you use the appropriate column (and possibly more than one column) to uniquely identify a record for deletion. Adjust the logic to match your database's schema.\n",
      "\n",
      "5. **Verify Deduplication:** After the database has been updated, run the `get_duplicate_records` method again to make sure no duplications remain.\n",
      "\n",
      "6. **Implement Ongoing Deduplication:** Consider automating this process or integrating it into your data ingestion pipeline to handle duplicates as new data comes in.\n",
      "\n",
      "Remember to test thoroughly in a non-production environment before deploying these changes to your live database. Also, you should always be aware of the transactional nature and the potential need to roll back changes if something doesn't go as planned.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(query_dup2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a reasonable approach! Using the time_stamp to determine the precedence of records is a common deduplication strategy, especially when dealing with dialogue data where the most recent entry might be the most relevant or up-to-date. By deleting the older duplicates, you ensure that the database maintains the latest information, which is generally beneficial for contextual learning and the performance of Large Language Models.\n",
      "\n",
      "Here's what you can do to implement this strategy:\n",
      "\n",
      "1. Modify the `MemoryHandler` class to include a method called `deduplicate_records`, which will:\n",
      "   - Fetch all records that are considered duplicates based on the similarity criteria you've established.\n",
      "   - Sort these records by their `time_stamp` in ascending order.\n",
      "   - Keep the most recent record (the last one in the sorted list) and delete all others.\n",
      "\n",
      "2. You should also ensure that there is a robust logging mechanism in place that records every deletion action for audit purposes. This way, in case something goes wrong, you have a log of what was deleted.\n",
      "\n",
      "3. It's also important to retain a backup of the database before performing any large-scale deletions. You can automate this process to create backups periodically or before deduplication runs.\n",
      "\n",
      "4. Test the `deduplicate_records` method thoroughly in a development or staging environment before deploying it to production. Include various scenarios to make sure the method works as intended and doesn’t accidentally delete non-duplicates.\n",
      "\n",
      "5. After implementing and testing, schedule this method to run at regular intervals that make sense for your application's data generation rate. Consider also implementing real-time checks during data insertion to prevent duplicates from being created in the first place.\n",
      "\n",
      "6. Lastly, monitor the deduplication process's impact on the LLM's contextual learning capabilities. Deduplication might influence the model's performance, so it's important to track metrics and ensure that the changes are beneficial.\n",
      "\n",
      "Keep in mind to do these steps carefully, especially if you're working with a live production database. Always prioritize data integrity and system stability.\n"
     ]
    }
   ],
   "source": [
    "query_dup3 = \"\"\"\n",
    "I think the provided code looks good. what we can do is to get the time_stamp of the duplicate_records and delete the older one. What do you think?\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "result = conversation_bufw.run(query_dup3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! The output you provided contains pairs of duplicate messages, but there aren't any timestamps included in the output. For the deduplication to work based on timestamps, we need to get not just the duplicate records but also their timestamps. We’ll need to modify both `get_duplicate_records` and `deduplicate_records` methods to include timestamp data. Below is an updated version of the `deduplicate_records` method which assumes that the `get_duplicate_records` method has been adjusted to return a list of tuples, where each tuple contains the record ID, the message, and the timestamp, sorted newest to oldest.\n",
      "\n",
      "First, ensure that your `get_duplicate_records` method returns data like this:\n",
      "\n",
      "```python\n",
      "[\n",
      "  ((record_id_1, 'I am going outside.', timestamp_1), (record_id_2, 'I am going outside again.', timestamp_2)),\n",
      "  # ... more duplicate pairs with their respective timestamps\n",
      "]\n",
      "```\n",
      "\n",
      "And here's your updated `deduplicate_records` function based on the assumption above:\n",
      "\n",
      "```python\n",
      "def deduplicate_records(self):\n",
      "    duplicates = self.get_duplicate_records()\n",
      "    with self.conn.cursor() as cursor:\n",
      "        for record_pair in duplicates:\n",
      "            # In the record_pair, the first one has an older timestamp and is the one we want to delete\n",
      "            record_to_delete_id = record_pair[0][0]\n",
      "\n",
      "            # Your deduplication logic here. For example, delete the older record:\n",
      "            delete_query = f\"DELETE FROM {self.db_config['table_name']} WHERE id = %s;\"\n",
      "            cursor.execute(delete_query, (record_to_delete_id,))\n",
      "\n",
      "        self.conn.commit()\n",
      "```\n",
      "\n",
      "Make sure to replace `id` with your actual primary key column name, and the entries in the tuple access (`record_pair[0][0]`) might differ depending on how you structured your data returned by `get_duplicate_records`.\n",
      "\n",
      "Please note that this deduplication logic assumes that you have IDs and timestamps for each record and that you've adjusted your method for fetching duplicates to include the timestamps and sort them accordingly. If the `get_duplicate_records` method does not already include this functionality, it will need to be modified to do so.\n"
     ]
    }
   ],
   "source": [
    "query_dup4 = \"\"\"\n",
    "\n",
    "I think the provided code looks good. what we can do is to get the time_stamp of the duplicate_records and delete the older one. \n",
    "\n",
    "Can you provide me code for this?\n",
    "\n",
    "def deduplicate_records(self):\n",
    "    duplicates = self.get_duplicate_records()\n",
    "    with self.cur as cursor, self.conn:\n",
    "        for record_pair in duplicates:\n",
    "            # Assumption: The first record of the pair is the one you want to keep\n",
    "            record_to_delete = record_pair[1]\n",
    "            \n",
    "            # Your deduplication logic here. For example, delete the record:\n",
    "            delete_query = f\"DELETE FROM {self.db_config['table_name']} WHERE message = %s AND chat_id = %s;\"\n",
    "            cursor.execute(delete_query, (record_to_delete, self.chat_id))\n",
    "\n",
    "Output of get_duplicate_records is [['I am going outside.', 'I am going outside again.']]\n",
    "\"\"\"\n",
    "\n",
    "result = conversation_bufw.run(query_dup4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_con = \"\"\"\n",
    "I am skipping duplicate_records for now. I will come back to it later. Now i want to move to Contradictory Information. Can you provide me code for this?\n",
    "I am providing you example for this.\n",
    "\n",
    "Contradictory Information:\n",
    "\n",
    "User: Whenever I go out to my car, remind me to fill up the screenwash.\n",
    "Agent: Okay, I will remind you to fill the screenwash when you go out to your car.\n",
    "\n",
    "User: Oh, also remind me to check the oil when I go out to the car.\n",
    "Agent: Okay, I will remind you to fill your screenwash and check your oil when you go out to your car.\n",
    "…\n",
    "User: I have filled my screenwash and checked my oil. Next time I go out to my car, remind me to check my tires.\n",
    "Agent: I will remind you to check your tires the next time you go out to your car.\n",
    "\n",
    "Note: There have been three directives for reminders. The second is additive to the first, but the third one contradicts the first two because the user has performed the original tasks. If all three of these events are in the LTM, which one would be chosen? We do not want the agent to remind the user about the screenwash and oil, but only the tires. \n",
    "\n",
    "\n",
    "Not Contradictory information:\n",
    "\n",
    "User: Hey, can you set up a reminder for me?\n",
    "Agent: Sure, what would you like to be reminded of?\n",
    "\n",
    "User: I want to be reminded to leave the house at 2pm today.\n",
    "Agent: Alright, I've set a reminder for you to leave the house at 2pm today. Anything else you need?\n",
    "\n",
    "User: Will you remind me again, when I have to leave the house at 6pm this evening?\n",
    "Agent: Absolutely, I've also set a reminder for you to leave the house at 6pm today. \n",
    "\n",
    "\n",
    "How to develop a tool for this using Langchain? Can you provide me code for this?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To resolve contradictory information in the context of reminders using the Langchain library, you'd want to build a system that can take in new information, update the existing context, and discard outdated or fulfilled tasks. Langchain is a convenient tool for building on top of large language models like GPT-4 and can integrate various components for a solution that incorporates memory and context understanding.\n",
      "\n",
      "Here's an approach to handling this using pseudo-code, as Langchain and its potential for such a niche application require a bit more nuanced implementation:\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.memory import PostgresMemory\n",
      "\n",
      "# Set up your LLM\n",
      "llm = OpenAI(api_key=\"your-api-key\")\n",
      "\n",
      "# Set up your memory storage\n",
      "postgres_memory = PostgresMemory(database_url=\"postgres://...\", table_name=\"PostGresMemoryHistory\")\n",
      "\n",
      "def handle_new_directive(user_input, current_reminders):\n",
      "    \"\"\"\n",
      "    This function takes in new user input and the current set of reminders and updates the reminders based on the new input.\n",
      "    \"\"\"\n",
      "    # You would run the input through the LLM to determine intent and details of the directive\n",
      "    response, action = llm.process(user_input)\n",
      "\n",
      "    # If action is to add a new reminder, it needs to be checked against existing reminders\n",
      "    if action.type == \"add_reminder\":\n",
      "        # Check if the reminder is contradictory or an update to existing reminders\n",
      "        updated_reminders = update_reminders(current_reminders, action.details)\n",
      "        \n",
      "        # Save updated reminders back to the memory\n",
      "        postgres_memory.save(updated_reminders)\n",
      "    \n",
      "    return response, updated_reminders\n",
      "\n",
      "def update_reminders(current_reminders, new_reminder_details):\n",
      "    \"\"\"\n",
      "    This function checks if the new reminder contradicts old ones and updates accordingly.\n",
      "    \"\"\"\n",
      "    # Check for contradictions in current_reminders and remove or update them\n",
      "    non_contradictory_reminders = []\n",
      "    for reminder in current_reminders:\n",
      "        if not is_contradictory(reminder, new_reminder_details):\n",
      "            non_contradictory_reminders.append(reminder)\n",
      "    \n",
      "    # Append the new reminder\n",
      "    non_contradictory_reminders.append(new_reminder_details)\n",
      "    \n",
      "    return non_contradictory_reminders\n",
      "\n",
      "def is_contradictory(existing_reminder, new_reminder_details):\n",
      "    \"\"\"\n",
      "    This function checks if the new reminder details contradict an existing reminder.\n",
      "    \"\"\"\n",
      "    # Logic to determine contradiction based on context and reminder details\n",
      "    # For instance, if reminders are about the same subject but with different actions,\n",
      "    # deem it as contradictory if the subject state has changed (e.g., task already done)\n",
      "    if existing_reminder.subject == new_reminder_details.subject and existing_reminder.action_done:\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "# Example to initiate the system with a user directive\n",
      "current_reminders = postgres_memory.retrieve()\n",
      "response, updated_reminders = handle_new_directive(\"Hey, remind me to check my tires when I go out to my car.\", current_reminders)\n",
      "\n",
      "# The response would be what the agent tells the user, and updated_reminders would be the latest set of reminders\n",
      "```\n",
      "In this pseudo-code:\n",
      "\n",
      "1. `handle_new_directive` is the function that processes new user directives.\n",
      "2. `update_reminders` checks for contradictions with current reminders and updates the reminder list.\n",
      "3. `is_contradictory` determines if one reminder contradicts another, which you would need to define based on your specific context and needs.\n",
      "\n",
      "This example assumes that `llm.process` and `action.type` are part of a hypothetical interface for interacting with the LLM and parsing the intent and details of user directives. In actual implementation, you will need to build or use existing functions to parse and understand user directives accurately.\n",
      "\n",
      "The notion of contradiction can be complex, and the logic will vary based on the specific use cases and context. You would also want to integrate Natural Language Understanding (NLU) components to interpret the semantics of reminders better. Building such a system would involve iterative development and testing with real user inputs to ensure the system interprets and handles reminders accurately.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(input_con)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem we are working on involves optimizing the memory management in a Large Language Model (LLM) system that integrates GPT-4 for contextual learning. Specifically, the system uses a PostgreSQL database to store the historical context used by the LLM to inform its responses. The goal is to refine the system through three main strategies:\n",
      "\n",
      "1. **Deduplication**: Ensuring that our database doesn't have redundant data, which saves space and improves performance.\n",
      "\n",
      "2. **Conflict Resolution**: Handling contradictory information, especially in dynamic contexts like reminders or tasks, where the user might change their mind or provide updated instructions.\n",
      "\n",
      "3. **Temporal Organization**: Keeping information in a structured temporal manner within the `PostGresMemoryHistory` class so that the most relevant and up-to-date information is readily accessible to the LLM.\n",
      "\n",
      "Our focus is to develop an efficient way for the LLM to process and recall information by improving how it stores and retrieves this data. This entails creating robust methods for dealing with new user directives, by checking against existing data to avoid duplication, resolving any conflicts that arise, and maintaining a coherent and updated timeline of events or tasks. The use of NLU (Natural Language Understanding) is crucial for accurately interpreting user directives, and iterative development with real user inputs is emphasized to ensure the system's effectiveness. The pseudo-code provided earlier outlines a potential approach for structuring this system.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(\"Tell me about the problem statement we are working on?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: My apologies for the oversight. In addition to deduplication, conflict resolution, and temporal organization, the fourth key strategy for memory management in a Large Language Model system like GPT-4 would be hierarchy management. This involves categorizing information based on its level of importance or relevance to the user's current context. \n",
      "\n",
      "The `PostGresMemoryHistory` class can be expanded with functions to prioritize information hierarchy. For instance, more frequently accessed or recent information can be made more readily available than older or less-used data. This can be accomplished through a tagging system or a scoring mechanism that ranks the memory entries. The class would need methods to adjust these rankings as the user interacts with the system and their priorities change.\n",
      "\n",
      "The hierarchy management aspect ensures the LLM can distinguish between what is essential to remember in the short term (e.g., an upcoming meeting) versus valuable knowledge to retain long term (e.g., user preferences or a learned skill). It acts as an additional filter to deliver the most contextually relevant information when prompts are issued by the user, enhancing the efficiency and effectiveness of the LLM's contextual learning capabilities.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(\"You missed the last memory type. There were 4 types of memory\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_id = \"0159735d-f1f4-4667-b28c-6572ba15ed90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_memory = PostGresMemoryHistory(db_host=os.environ.get('db_host'), db_name=os.environ.get('db_name'), db_user=os.environ.get('db_username'), db_password=os.environ.get('db_password'), db_port=os.environ.get('db_port'), table_name=os.environ.get('conversation_table_name'), chat_id=chat_id)\n",
    "memory  = ConversationSummaryMemory.from_messages(chat_memory=postgres_memory, llm=llms, return_messages=True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are the PromptTemplates for each memory type considering the inputs you've provided:\n",
      "\n",
      "---\n",
      "\n",
      "**Chat History Memory Management**\n",
      "\n",
      "1. Deduplication PromptTemplate:\n",
      "   ```\n",
      "   AI, please review the chat history and identify any duplicated information. Remove the redundancies while retaining the original context and ensure the conversation flow remains coherent.\n",
      "\n",
      "2. Conflict Resolution PromptTemplate:\n",
      "   ```\n",
      "   AI, analyze the chat history for any contradictory statements. Provide a resolution by considering the most recent and relevant information, and update any associated facts to reflect this resolution.\n",
      "\n",
      "3. Temporal Organization PromptTemplate:\n",
      "   ```\n",
      "   AI, reorganize the chat history chronologically. Assign timestamps to each statement or question where possible and reorder the chat to maintain a logical timeline, aiding in proper contextual understanding.\n",
      "\n",
      "4. Hierarchy Management PromptTemplate:\n",
      "   ```\n",
      "   AI, categorize the chat history based on the importance of the topics discussed. Highlight key information and prioritize it for quick reference in future interactions, adapting to the user's current context.\n",
      "\n",
      "---\n",
      "\n",
      "**Direct Input Memory Management**\n",
      "\n",
      "1. Deduplication PromptTemplate:\n",
      "   ```\n",
      "   AI, upon receiving a new input, cross-reference it with existing data. If duplication is detected, consolidate the information, ensuring the input database remains concise without loss of critical details.\n",
      "\n",
      "2. Conflict Resolution PromptTemplate:\n",
      "   ```\n",
      "   AI, when processing a new input, check for conflicts with existing data. Resolve by privileging the most accurate or updated information, and adjust the memory records to avoid future discrepancies.\n",
      "\n",
      "3. Temporal Organization PromptTemplate:\n",
      "   ```\n",
      "   AI, integrate new inputs by placing them into the correct temporal context within our memory structure. Ensure that this input is easily accessible and searchable by its relevant date or event context.\n",
      "\n",
      "4. Hierarchy Management PromptTemplate:\n",
      "   ```\n",
      "   AI, evaluate the new input for its significance to the user's needs and rank it accordingly within our memory hierarchy. Make sure that more critical information is more readily available than less important data.\n",
      "\n",
      "---\n",
      "\n",
      "These PromptTemplates are designed to guide the AI in managing chat history and direct inputs effectively. By using these templates, the AI can keep the memory database optimized for efficient retrieval and use, promoting better contextual learning and memory management within the LLM system.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(f\"{input}\\nFrom the text. For Handling the memories. Can you Provided me PromptTemplate for each memory type? There is are two types of Input 1. is chat history 2. Input.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \"\"\"\n",
    "\n",
    "Whats wrong in this code and how to fix it?\n",
    "\n",
    "\n",
    "\n",
    "postgres_memory = PostGresMemoryHistory(db_host=os.environ.get('db_host'), db_name=os.environ.get('db_name'), db_user=os.environ.get('db_username'), db_password=os.environ.get('db_password'), db_port=os.environ.get('db_port'), table_name=os.environ.get('conversation_table_name'), chat_id=chat_id)\n",
    "\n",
    "memory  = ConversationSummaryBufferMemory(chat_memory=postgres_memory, llm=llms,k=6, return_messages=True,verbose=True)\n",
    "conversation_bufw = ConversationChain(\n",
    "    llm=llms, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class MemoryPromptHandler:\n",
    "    \n",
    "    DUPLICATE_MEMORY_HANDLE_PROMPT =\"\"\"\n",
    "    From the conversation history, {history} and {input}.\n",
    "    \n",
    "    Your Task is to review the chat history and identify any duplicated information and combine the context of the repeated events into a single event. While retaining the original context,Your Goal is to create a smooth and enriching conversational experience.\n",
    "    \n",
    "    Your Response Should based on conversation with human by impliemnting above Instructions.\n",
    "    \n",
    "    If needed information about timestamps you can use Database tool to retrive the information.\n",
    "    \n",
    "    Here is the example of the conversation history.\n",
    "    \n",
    "    User: I am going outside.\n",
    "    AI: Have a good time outside!\n",
    "\n",
    "    User: I have returned home and am going to take a bath\n",
    "    AI: Enjoy your bath!\n",
    "\n",
    "    User: I am going outside again.\n",
    "    Agent: Thanks for letting me know, enjoy your time outside.\n",
    "    \n",
    "    Here: \"I am going outside again\" is repeated. So, you should combine the context of the repeated events into a single event.like: going outside\n",
    "\n",
    "    If you are not sure about the answer, truthfully say you are not sure.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CONTRADICTORY_MEMORY_HANDLE_PROMPT =\"\"\"\n",
    "    From the conversation history, {history} and {input}.\n",
    "    \n",
    "    Your Task is to review the chat history and identify any contradictory statements and resolve the contradiction.Provide a resolution by considering the most recent and relevant information, and update any associated facts to reflect this resolution. While retaining the original context,Your Goal is to create a smooth and enriching conversational experience.\n",
    "    \n",
    "    If needed information about timestamps you can use Database tool to retrive the information.\n",
    "    \n",
    "    Your Response Should based on conversation with human by impliemnting above Instructions.\n",
    "    \n",
    "    Here is the example of the conversation history.\n",
    "    \n",
    "    Contradictory information:\n",
    "    \n",
    "    User: Whenever I go out to my car, remind me to fill up the screenwash.\n",
    "    AI: Okay, I will remind you to fill the screenwash when you go out to your car.\n",
    "\n",
    "    User: Oh, also remind me to check the oil when I go out to the car.\n",
    "    AI: Okay, I will remind you to fill your screenwash and check your oil when you go out to your car.\n",
    "    \n",
    "    User: I have filled my screenwash and checked my oil. Next time I go out to my car, remind me to check my tires.\n",
    "    AI: I will remind you to check your tires the next time you go out to your car.\n",
    "\n",
    "    Note: There have been three directives for reminders. The second is additive to the first, but the third one contradicts the first two because the user has performed the original tasks. If all three of these events are in the LTM, which one would be chosen? We do not want the agent to remind the user about the screenwash and oil, but only the tires. \n",
    "\n",
    "    Not Contradictory information:\n",
    "\n",
    "    User: Hey, can you set up a reminder for me?\n",
    "    AI: Sure, what would you like to be reminded of?\n",
    "\n",
    "    User: I want to be reminded to leave the house at 2pm today.\n",
    "    AI: Alright, I've set a reminder for you to leave the house at 2pm today. Anything else you need?\n",
    "\n",
    "    User: Will you remind me again, when I have to leave the house at 6pm this evening?\n",
    "    Ai: Absolutely, I've also set a reminder for you to leave the house at 6pm today. \n",
    "    \n",
    "    Here: There are two reminders, but they are not contradictory. The first reminder is for 2pm, and the second is for 6pm. The agent should remind the user about both of these events.\n",
    "    \n",
    "    If you are not sure about the answer, truthfully say you are not sure.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    TEMPORARY_MEMORY_HANDLE_PROMPT =\"\"\"\n",
    "    From the conversation history, {history} and {input}.\n",
    "    \n",
    "    Your Task is to review the chat history and identify any temporary information and remove the temporary information from the conversation. While retaining the original context,Your Goal is to create a smooth and enriching conversational experience.\n",
    "    \n",
    "    If needed information about timestamps you can use Database tool to retrive the information.\n",
    "    \n",
    "    Your Response Should based on conversation with human by impliemnting above Instructions.\n",
    "    \n",
    "    Here is the example of the conversation history.\n",
    "    \n",
    "    Temporary Information:\n",
    "    User: Remember this shopping list for me: Bread, Eggs, and Milk\n",
    "    AI: I have remembered your shopping list of Bread, Eggs, and Milk\n",
    "\n",
    "    User: I have bought some eggs.\n",
    "    AI: Okay, I have removed the Eggs from your shopping list.\n",
    "\n",
    "    User: What is my shopping list again?\n",
    "    AI: On your shopping list you have: Bread and Milk\n",
    "\n",
    "    Note: A shopping list is piece of information that is very volatile, so the agent and its memory should be reactive to additions and deletions, returning only the latest version of the shopping list when asked.\n",
    "\n",
    "    User: When I return home from work, suggest a meal I can make for dinner.\n",
    "    AI: I will remember to suggest a meal that you can make for dinner when you get in from work.\n",
    "\n",
    "    User: I have preplanned meals this week, so for this week, don’t make any suggestions about what to make for dinner when I get home from work.\n",
    "    AI: All right, for this week, I will not suggest meals for you when you return from work. \n",
    "\n",
    "    User: I have finished my preplanned meals, go back to suggesting meals for me.\n",
    "    AI: I will resume suggesting meals for you when you return from work\n",
    "\n",
    "    Note: We have given the agent some directive in the past, then given it a contradictory instruction with an explicit understanding that this instruction will be temporary. Have the LTM deal with this temporary change of the instructions so that the agent can achieve the temporary task, and then go back to original instructions.\n",
    "\n",
    "    If you are not sure about the answer, truthfully say you are not sure.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    EPISODIC_MEMORY_HANDLE_PROMPT =\"\"\"\n",
    "    From the conversation history, {history} and {input}.\n",
    "    \n",
    "    Your Task is to review the chat history and identify any episodic memories and use the episodic memories to provide contextually rich and engaging interactions, facilitating a meaningful and continuous dialogue. While retaining the original context,Your Goal is to create a smooth and enriching conversational experience.\n",
    "    \n",
    "    If needed information about timestamps you can use Database tool to retrive the information.\n",
    "    \n",
    "    Your Response Should based on conversation with human by impliemnting above Instructions.\n",
    "    \n",
    "    Here is the example of the conversation history.\n",
    "    \n",
    "    Episodic Memories:\n",
    "    \n",
    "    Two days ago:\n",
    "    User: I have bought a side dresser for the bedroom, and some ingredients for dinner tonight. I am going to make omelets!\n",
    "    AI: That's great, you needed a side dresser for your bed, and omelets are a healthy dinner!\n",
    "    Today:\n",
    "    User: I think I will prepare something for dinner tonight… Do you think I could use the ingredients I bought two days ago?\n",
    "    AI: The eggs should be fine, unless you actually made and ate the omelets then!\n",
    "\n",
    "    Yesterday:\n",
    "    User: I have purchased some items for the house: A mirror, two potted plants and a standing lamp.\n",
    "    AI: Okay, hope you enjoy decorating your home!\n",
    "\n",
    "    Today:\n",
    "    User: Yesterday I got home with some new items for the house. Do you remember what they were?\n",
    "    AI: Sure! You bought A mirror, two potted plants and a standing lamp.\n",
    "\n",
    "\n",
    "    Note: Episodic retrieval allows the agent to order its memories through time, and recall or filter them using timestamps to answer questions with a time based component. \n",
    "    \n",
    "    If you are not sure about the answer, truthfully say you are not sure.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    DATABASE_REFRENCE_PROMPT =\"\"\"\n",
    "    \n",
    "    Your Goal is to assist other agents in the conversation by providing them with the information they need to respond to the user.\n",
    "    \n",
    "    System: {input}\n",
    "    AI:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "class MemoryAgentTool:\n",
    "    def __init__(self,llm, memory):\n",
    "        self.memory_prompt_handler = MemoryPromptHandler()\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "        \n",
    "    def handle_duplicate_memory(self):\n",
    "        conversation = ConversationChain(\n",
    "            llm=self.llm, \n",
    "            memory=ReadOnlySharedMemory(self.memory),\n",
    "            prompt=self.memory_prompt_handler.DUPLICATE_MEMORY_HANDLE_PROMPT\n",
    "        )\n",
    "        return conversation\n",
    "    \n",
    "    def handle_temporal_memory(self):\n",
    "        conversation = ConversationChain(\n",
    "            llm=self.llm, \n",
    "            memory=ReadOnlySharedMemory(self.memory),\n",
    "            prompt=self.memory_prompt_handler.DUPLICATE_MEMORY_HANDLE_PROMPT\n",
    "        )\n",
    "        return conversation\n",
    "    \n",
    "    def handle_episodic_memory(self):\n",
    "        conversation = ConversationChain(\n",
    "            llm=self.llm, \n",
    "            memory=ReadOnlySharedMemory(self.memory),\n",
    "            prompt=self.memory_prompt_handler.DUPLICATE_MEMORY_HANDLE_PROMPT\n",
    "        )\n",
    "        return conversation\n",
    "    \n",
    "    def handle_contradictory_memory(self):\n",
    "        conversation = ConversationChain(\n",
    "            llm=self.llm, \n",
    "            memory=ReadOnlySharedMemory(self.memory),\n",
    "            prompt=self.memory_prompt_handler.DUPLICATE_MEMORY_HANDLE_PROMPT\n",
    "        )\n",
    "        return conversation\n",
    "    \n",
    "    # def handle_database_refrence(self):\n",
    "    #     conversation = ConversationChain(\n",
    "    #         llm=llms, \n",
    "    #         memory=ReadOnlySharedMemory(memory),\n",
    "    #         prompt=self.memory_prompt_handler.DATABASE_REFRENCE_PROMPT\n",
    "    #     )\n",
    "    #     return conversation\n",
    "\n",
    "memory_agent_tool = MemoryAgentTool( llm=llms, memory=memory)\n",
    "\n",
    "duplicate_memory_tool = memory_agent_tool.handle_duplicate_memory\n",
    "contradictory_memory_tool = memory_agent_tool.handle_contradictory_memory\n",
    "temporal_memory_tool = memory_agent_tool.handle_temporal_memory\n",
    "episodic_memory_tool = memory_agent_tool.handle_episodic_memory\n",
    "\n",
    "tool = [\n",
    "    Tool(\n",
    "        name=\"Duplicate Memory Tool\",\n",
    "        description=\"good in handling duplicate memory\",\n",
    "        func=duplicate_memory_tool.run\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Contradictory Memory Tool\",\n",
    "        description=\"good in handling contradictory memory\",\n",
    "        func=contradictory_memory_tool.run,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Temporal Memory Tool\",\n",
    "        description=\"good in handling temporal memory\",\n",
    "        func=temporal_memory_tool.run,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Episodic Memory Tool\",\n",
    "        description=\"good in handling episodic memory\",\n",
    "        func=episodic_memory_tool.run,\n",
    "    ),\n",
    "    # Tool(\n",
    "    #     name=\"Database Refrence Tool\",\n",
    "    #     description=\"good in extracting data from database and provide refrence to other agents\",\n",
    "    #     agent=memory_agent_tool.handle_database_refrence(),\n",
    "    # ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent_type = AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    llm=llms,\n",
    "    memory=memory,\n",
    "    tools=tool,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"I am going outside.\")\n",
    "\n",
    "\n",
    "I have used this refrence \n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"input\", \"chat_history\"], template=template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "summary_chain = LLMChain(\n",
    "    llm=OpenAI(),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=readonlymemory,  # use the read-only memory to prevent the tool from modifying the memory\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Summary\",\n",
    "        func=summary_chain.run,\n",
    "        description=\"useful for when you summarize a conversation. The input to this tool should be a string, representing who will read this summary.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "and my error is \n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "Cell In[42], line 12\n",
    "      5 temporal_memory_tool = memory_agent_tool.handle_temporal_memory\n",
    "      6 episodic_memory_tool = memory_agent_tool.handle_episodic_memory\n",
    "      8 tool = [\n",
    "      9     Tool(\n",
    "     10         name=\"Duplicate Memory Tool\",\n",
    "     11         description=\"good in handling duplicate memory\",\n",
    "---> 12         func=duplicate_memory_tool.run\n",
    "     13     ),\n",
    "     14     Tool(\n",
    "     15         name=\"Contradictory Memory Tool\",\n",
    "     16         description=\"good in handling contradictory memory\",\n",
    "     17         func=contradictory_memory_tool.run,\n",
    "     18     ),\n",
    "     19     Tool(\n",
    "     20         name=\"Temporal Memory Tool\",\n",
    "     21         description=\"good in handling temporal memory\",\n",
    "     22         func=temporal_memory_tool.run,\n",
    "     23     ),\n",
    "     24     Tool(\n",
    "     25         name=\"Episodic Memory Tool\",\n",
    "     26         description=\"good in handling episodic memory\",\n",
    "     27         func=episodic_memory_tool.run,\n",
    "...\n",
    "     41     verbose=True\n",
    "     42 )\n",
    "     44 agent.run(\"I am going outside.\")\n",
    "\n",
    "AttributeError: 'function' object has no attribute 'run'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the provided code snippet, there are a few potential issues that need to be addressed. Here's a breakdown of each line and possible problems with suggestions for fixing them:\n",
      "\n",
      "1. `postgres_memory` Initialization:\n",
      "   ```\n",
      "   postgres_memory = PostGresMemoryHistory(db_host=os.environ.get('db_host'), db_name=os.environ.get('db_name'), db_user=os.environ.get('db_username'), db_password=os.environ.get('db_password'), db_port=os.environ.get('db_port'), table_name=os.environ.get('conversation_table_name'), chat_id=chat_id)\n",
      "   ```\n",
      "   Potential Issues:\n",
      "   - Environment variables must be set before use. If not set, `os.environ.get()` will return `None`.\n",
      "   - The `chat_id` is referenced but not defined in the code snippet.\n",
      "   \n",
      "   Fixes:\n",
      "   - Ensure that all the necessary environment variables (`db_host`, `db_name`, `db_username`, `db_password`, `db_port`, `conversation_table_name`) are set correctly within your environment.\n",
      "   - Define the `chat_id` variable before using it in the code or make sure it is passed to the script correctly.\n",
      "\n",
      "2. `ConversationSummaryBufferMemory` Initialization:\n",
      "   ```\n",
      "   memory = ConversationSummaryBufferMemory(chat_memory=postgres_memory, llm=llms,k=6, return_messages=True,verbose=True)\n",
      "   ```\n",
      "   Potential Issues:\n",
      "   - The variable `llms` is used but not defined in the code snippet. It should represent the Large Language Model (LLM) that you are using.\n",
      "   - The `k` parameter is set to 6, which might be intended, but you should ensure it aligns with the required buffer size.\n",
      "   \n",
      "   Fixes:\n",
      "   - Define the `llms` variable to reference the LLM that you're working with.\n",
      "   - Confirm that the `k` parameter value is set correctly for the intended behavior.\n",
      "\n",
      "3. `ConversationChain` Initialization:\n",
      "   ```\n",
      "   conversation_bufw = ConversationChain(\n",
      "       llm=llms, \n",
      "       memory=memory\n",
      "   )\n",
      "   ```\n",
      "   Potential Issues:\n",
      "   - If `ConversationChain` is a class that you have written or is part of a library you are using, ensure that it is imported properly at the beginning of the script.\n",
      "   - The variable `conversation_bufw` seems to be a typo or a misname; typically, it should be something like `conversation_buffer`.\n",
      "   - Make sure the `ConversationChain` class is designed to accept only these two parameters (`llm` and `memory`) and does not require additional ones.\n",
      "   \n",
      "   Fixes:\n",
      "   - Import `ConversationChain` if it's a separate class.\n",
      "   - Rename `conversation_bufw` to `conversation_buffer` or another appropriate variable name, if it was a typo.\n",
      "   - Check the `ConversationChain` class's constructor to ensure all required parameters are being passed.\n",
      "\n",
      "If you apply these fixes and ensure that all external dependencies, such as environment variables and necessary imports, are correctly set up, the code should function as intended. Having a full traceback of any errors that occur when running the code would provide more specific guidance on what could be wrong.\n"
     ]
    }
   ],
   "source": [
    "result = conversation_bufw.run(ques)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
